{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ACIS Insurance Risk Analytics â€“ Task 2\n",
                "\n",
                "**Objective:** The goal of this notebook is to clean, perform feature engineering, prepare the data for modeling, and version the processed dataset using DVC."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add the src directory to the path\n",
                "sys.path.append(os.path.abspath('../src'))\n",
                "\n",
                "from data_loader import load_raw_data\n",
                "from dvc_utils import dvc_steps\n",
                "\n",
                "df = load_raw_data('../data/MachineLearningRating_v3.txt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df is not None:\n",
                "    # 1. Loss Ratio\n",
                "    # Avoid division by zero\n",
                "    df['LossRatio'] = df.apply(lambda row: row['TotalClaims'] / row['TotalPremium'] if row['TotalPremium'] != 0 else 0, axis=1)\n",
                "\n",
                "    # 2. Claim Severity\n",
                "    # Avoid division by zero if NumberOfClaims is available, else maybe skip or assume 0\n",
                "    # Assuming 'NumberOfClaims' may not be explicit in the prompt columns, checking or creating if specific column name is known.\n",
                "    # If not present, we can't created it exactly as requested without that column. \n",
                "    # But assuming standard dataset features, let's look for a proxy or column.\n",
                "    # If 'NumberOfClaims' does not exist, we check implied columns. \n",
                "    # Prompt asked for: ClaimSeverity = TotalClaims / NumberOfClaims\n",
                "    # Let's assume the column exists or we handle it safely.\n",
                "    if 'NumberOfClaims' in df.columns:\n",
                "         df['ClaimSeverity'] = df.apply(lambda row: row['TotalClaims'] / row['NumberOfClaims'] if row['NumberOfClaims'] > 0 else 0, axis=1)\n",
                "    else:\n",
                "        print(\"Warning: 'NumberOfClaims' column not found. Skipping ClaimSeverity calculation.\")\n",
                "\n",
                "    # 3. Vehicle Age\n",
                "    # VehicleAge = CurrentYear - RegistrationYear\n",
                "    current_year = 2025 # Or use pd.Timestamp.now().year\n",
                "    if 'RegistrationYear' in df.columns:\n",
                "        # Clean RegistrationYear first if needed\n",
                "        df['VehicleAge'] = current_year - pd.to_numeric(df['RegistrationYear'], errors='coerce')\n",
                "        df['VehicleAge'] = df['VehicleAge'].fillna(0) # or median\n",
                "        df.loc[df['VehicleAge'] < 0, 'VehicleAge'] = 0 # Handle future dates error\n",
                "    else:\n",
                "        print(\"Warning: 'RegistrationYear' column not found.\")\n",
                "    \n",
                "    print(\"New features created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df is not None:\n",
                "    # Missing Values\n",
                "    # Numeric: Median\n",
                "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "    for col in numeric_cols:\n",
                "        median = df[col].median()\n",
                "        df[col] = df[col].fillna(median)\n",
                "    \n",
                "    # Categorical: Mode\n",
                "    cat_cols = df.select_dtypes(include=['object']).columns\n",
                "    for col in cat_cols:\n",
                "        if not df[col].mode().empty:\n",
                "            mode = df[col].mode()[0]\n",
                "            df[col] = df[col].fillna(mode)\n",
                "\n",
                "    # Outlier handling (Simple Capping for demo)\n",
                "    # Cap TotalClaims at 99th percentile\n",
                "    cap_val = df['TotalClaims'].quantile(0.99)\n",
                "    df.loc[df['TotalClaims'] > cap_val, 'TotalClaims'] = cap_val\n",
                "    \n",
                "    print(\"Cleaning completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df is not None:\n",
                "    # Label Encoding for high cardinality or One-Hot for low\n",
                "    # Using pandas get_dummies for simplicity or factorize\n",
                "    \n",
                "    cols_to_encode = ['Make', 'Province', 'VehicleType']\n",
                "    for col in cols_to_encode:\n",
                "        if col in df.columns:\n",
                "            # Simple Label Encoding equivalent\n",
                "            df[col + '_Encoded'] = pd.factorize(df[col])[0]\n",
                "    \n",
                "    print(\"Encoding completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "if df is not None:\n",
                "    scaler = StandardScaler()\n",
                "    # Scaling selected numeric columns (e.g. TotalPremium, TotalClaims, SumInsured)\n",
                "    scale_cols = ['TotalPremium', 'TotalClaims', 'SumInsured']\n",
                "    # Ensure they exist\n",
                "    scale_cols = [c for c in scale_cols if c in df.columns]\n",
                "    \n",
                "    if scale_cols:\n",
                "        df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
                "        print(\"Scaling completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Clean Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "output_dir = '../data/processed'\n",
                "if not os.path.exists(output_dir):\n",
                "    # Check locally\n",
                "    if not os.path.exists('data/processed'):\n",
                "        os.makedirs('data/processed', exist_ok=True)\n",
                "        output_dir = 'data/processed'\n",
                "    else:\n",
                "        output_dir = 'data/processed'\n",
                "\n",
                "if df is not None:\n",
                "    output_path = os.path.join(output_dir, 'cleaned_data.csv')\n",
                "    df.to_csv(output_path, index=False)\n",
                "    print(f\"Saved processed data to {output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df is not None:\n",
                "    print(f\"New Shape: {df.shape}\")\n",
                "    print(\"New Columns Created:\", [c for c in df.columns if 'Encoded' in c or c in ['LossRatio', 'ClaimSeverity', 'VehicleAge']])\n",
                "    display(df.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DVC Versioning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(dvc_steps())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !dvc add data/processed/cleaned_data.csv\n",
                "# !git add data/processed/cleaned_data.csv.dvc\n",
                "# !git commit -m \"task-2: add processed dataset with new features\"\n",
                "# !dvc push"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Closing Notes\n",
                "The data is now cleaned, feature-engineered, and scaled, making it ready for modeling in Task 3. We have also set up DVC steps to ensure reproducible data versioning."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}